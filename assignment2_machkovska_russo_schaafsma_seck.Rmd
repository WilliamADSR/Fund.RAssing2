---
title: "Logistic regression assignment, group 3"  
author: Elizabeth Machkovska, Letícia Marçal Russo, Madio Seck, William Schaafsma 
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
       highlight: textmate
       theme: flatly
       number_sections: yes
       toc: yes
       toc_float:
         collapsed: yes
         smooth_scroll: no
---

# Context 

# Packages
```{r libraries, warning = FALSE, message = FALSE}
library(ggplot2)
library(GGally)
library(readr)
library(dbplyr)
library(tidyverse)
library(magrittr)
library(lattice)
library(correlation)
library(ggcorrplot)
library(see)
library(caret)
library(greybox)
library(summarytools)
library(DAAG)
library(fastDummies)
```

# Research question and relevance 

- tell that we are doing an exploratory analysis about the variables that could predict better heart disease. Therefore, we are not working with hypothesis 

- when running test statistics, we will work with an alpha of 0.05 and confidence of 95%.

- our goal is to make good predictions and not focus in inference. Mainly see how our model works in new data. We are will do it testing predictions in a test set that we will create in the beginning of the analysis.  

# Loading the Dataset
The data set is derived from Kaggle and is uploaded by fedesoriano (2021). With the following script a distinction is made between continuous (numeric) and categorical variables which are transformed to factor type data. 
```{r}
heart <- read_csv("heart.csv", 
                  col_types = cols(Age = col_number(),
                                   Sex = col_factor(),
                                   ChestPainType = col_factor(),
                                   RestingBP = col_number(),
                                   Cholesterol = col_number(), 
                                   FastingBS = col_factor(), 
                                   RestingECG = col_factor(),
                                   MaxHR = col_number(),
                                   ExerciseAngina = col_factor(),
                                   Oldpeak = col_number(),
                                   ST_Slope = col_factor(), 
                                   HeartDisease = col_factor()
                   )
                  )
heart$FastingBS <- factor(heart$FastingBS,
                                labels = c("B120", "A120"))
heart$HeartDisease <- factor(heart$HeartDisease,
                                labels = c("Norm", "Prob"))
```

# Explanation of the variables

This data set holds 12 variables of which **Heart disease** is our dichotomous outcome variable. A futher elaboration on the variables follows:

**Variable 1**: *Age*. It's a numeric variable of the patients' age expressed in years.  
  

**Variable 2**: *Sex*. It's a categorical variable which holds the groups: F = female & M = male.  
  

**Variable 3**: *ChestPaintype*. It's a categorical variable which contains 4 different types of chest pains.  

*TA*: Typical Angina, a type of chest pain caused by reduced blood flow to the heart.  

*ATA*: Atypical Angina, a type of chest pain that doesn't meet the criteria for angina.  

*NAP*: Non-Anginal Pain, a type of chest pain that is not caused by heart disease or a heart attack. 

*ASY*: Asymptomatic, a type of chest pain that includes no chest pain but there is angina.  


**Variable 4**: *RestingBP*. It's a numeric variable representing patients' resting blood pressure (mmHg). The average blood pressure for an adult male is 120/80 mmHg. Average blood pressure increases when people get older. Furthermore, women's average blood pressure is mostly a little bit lower compared to males. A resting blood pressure > 140 is considered high and may cause heart failure.  


**Variable 5**: *Cholesterol*. It's a numeric variable representing patients' cholesterol levels (mm / dl). A healthy adult has a cholesterol level lower than 200 mm / dl. High cholesterol levels (> 240) is moderate/high and therefore may cause heart failure.  


**Variable 6**: *FastingBS*. It's a dichotomous variable representing patients' fasting blood sugar (mg / dl). Fasting blood sugar levels > 120 mg / dl are coded as a 1. Fasting blood sugar levels < 120 mg / dl are coded as a 0. High levels of blood sugar infer diabetes which may cause heart failure.   


**Variable 7**: *RestingECG*. It's a categorical variable which contains 3 different types of electric activity in the heart.  

*Normal*: The heart is beating in a regular sinus rhythm at a normal pace. No danger whatsoever.   

*ST*: Indicates an abnormality in the heartbeat. It is associated with increased cardiovascular risk.  

*LVH*: This activity in the heart represents valve problems which increasingly thickens the wall of the heart's main pumping chamber. It's the most common cause of high blood pressure which may cause heart failure.  


**Variable 8**: *MaxHR*.  


**Variable 9**: *Exercise Angina*.  


**Variable 10**: *Oldpeak*.  


**Variable 11**: *ST_slope*.  


**Variable 12**: *HeartDisease*.  

# Inspecting the dataset and cleaning up

add information like:
* number of observations

```{r} 
head(heart) %>% 
  knitr::kable(format = "markdown", digits= 1, padding = 30, align = 'c')
```
Apparently, none of the variables have missing values, but we will get more insights in the descriptive analysis.

```{r}
summary(heart)
```
Comment here

```{r}
str(heart)
```

# Descriptive analysis

Comment here.

```{r}
Hist_age <- hist(heart$Age)
```

This is not working. Check why.
```{r}
#Box_age <- boxplot(heart$Age) + title(main = "Distribution of age", ylab = "Age in years")
```

Comment here.

```{r}
median(heart$Age)
```

Age is distributed normally. The youngest observations is 28 years old and the oldest observation is 77 years old. The median of this data set is 54.

```{r}
Hist1_chol <- hist(heart$Cholesterol)
```

The Cholesterol variable has 172 observations of the number 0. It is not possible that a person has zero cholesterol level. Therefore, we will consider this information as error and, consequently, missing values.
In the next topic, we will make a decision about how to deal with that.


```{r}
#Box_chol <- boxplot(heart$Cholesterol) 
#            + title(main = "Distribution of cholesterol levels", 
#                    ylab = "Cholesterol levels (mm / dl)")
```
Comment here.

```{r}
min(heart$Cholesterol)
max(heart$Cholesterol)
median(heart$Cholesterol)
```

Here we see that the distribution of cholesterol levels has several outliers. The median is a cholesterol level of 237 which is within the range of a healthy level. a The lowest level of cholesterol is 85 and the highest level of cholesterol is 603. When checking assumptions we will see whether we need to exclude more observations due to abnormal cholesterol levels.

```{r}
Hist_BPrest <- hist(heart$RestingBP)
```

Comment here.

```{r}
#Box_BPrest <- boxplot(heart$RestingBP) 
#                 + title(main = "Distribution of resting blood pressure levels", 
#                         ylab = "Resting blood pressure levels (mmHg)")
```

Comment here.

```{r}
median(heart$RestingBP)

max(heart$RestingBP)
```

Most of the observations have a healthy resting blood pressure, which the median of 130 also represents. There are some outliers past the maximum of the boxplot, the highest observations has a resting blood pressure of 200 which is very dangerous.

## Missing values for cholesterol

-decision if we delete missing values or delete the variable
- 172 is almost 20% of the dataset (918 observations)
-run a correlation here to see if cholesterol is related to heart disease 
-argument that the best choice was to delete the variable


```{r}
# Excluding the 0 values
heart_cholesterol <- heart %>% 
                     filter(!Cholesterol == 0)
```

Correlation

```{r}
heart_cholesterol %>%
  select(HeartDisease, Cholesterol) %>% 
  correlation(include_factors = TRUE, method = "auto") %>%
  summary() %>%
    plot(show_labels=TRUE,
       show_p=TRUE,
       size_point=5,
       size_text=4,
       digits=2,
       type="tile",
       )
```
Very weak correlation. Decision: exclude the variable.

```{r}
Clean_Heart <- heart %>% 
               select(!Cholesterol)
```

# Split the data into training and test samples
In the training data set 80% (p=0.8) of the rows were generated as a new sample. Out of 735 observations 45% has normal heart performance (328 rows) and 55% has heart problems (407 rows). Therefore, the training data has approximately equal distribution of classes within response variable. 
```{r}
set.seed(42)

# Split data randomly
trainDataIndex <- createDataPartition(Clean_Heart$HeartDisease, 
                                      p = 0.8, 
                                      list = FALSE) 

# Create training and testing set
train_heart <- Clean_Heart[trainDataIndex, ]
test_heart <- Clean_Heart[-trainDataIndex, ]

table(train_heart$HeartDisease)
```

# Correlation 

The **easystats** package by Makowski and his colleagues (2019) and the **greybox** package is used to create the pairwise correlation matrix. The long variable names are first shortened to make visualization neater. 
```{r}
# Make the names shorter
train_heart$CPT <- train_heart$ChestPainType
train_heart$R_BP <- train_heart$RestingBP
train_heart$F_BS <- train_heart$FastingBS
train_heart$R_ECG <- train_heart$RestingECG
train_heart$E_A <- train_heart$ExerciseAngina
train_heart$ST_S <- train_heart$ST_Slope
train_heart$H_D <- train_heart$HeartDisease

```

## Continuous predictors
Correlation between continuous predictor variables and dichotomous categorical outcome variable is calculated using the *Point-biserial* correlation. Regarding nominal variables, the coefficient does tell something about the strength of a relation, however, not about the direction, even though those are visualised in the matrix for *H_D*. The relationship between continuous predictors is measured using *Pearson*. 
```{r, message=FALSE}
train_heart %>%
  select(Age, R_BP, MaxHR, Oldpeak, H_D) %>% 
  correlation(include_factors = TRUE, method = "auto") %>%
  summary() %>%
    plot(show_labels=TRUE,
       show_p=TRUE,
       size_point=5,
       size_text=4,
       digits=2,
       type="tile",
       )
```

What is this code for?
summary(train_heart)
hist_oldpeak <- hist(train_heart$Oldpeak)

When comparing the coefficients for the observations of patients with a heart disease (*H_D.Prob*), in order from strongest to weakest association, there is a significant strong positive relationship between oldpeak, which means that the higher the value of *O_P*, the more often a heart problem occurs. Relationship between *MaxHR* and heart disease is moderately negative, which suggests that the higher the maximum heart rate achieved the less heart problems are observed. Both *Oldpeak* and *MaxHR* have a moderate strength of a relationship between each other and with *Age*. In turn, *Age* has a moderate positive relationship with heart problems, meaning that the older the patients observed, the more often they have a heart disease detected. Association between resting blood pressure (*R_BP*) is weak and positive. The relationships have a reverse direction when comparing the values for a normal heart (*H_D.Norm*). 
From continuous predictor variables the values of cholesterol (*Chol*) have insignificant negligible association with both the heart disease and other numeric predictors. 

## Categorical predictors
The *Cramer's V* is used for categorical variables. 
```{r, message=FALSE}
train_heart %>%
  select(CPT, R_ECG, ST_S, H_D) %>% 
  assoc() %>%
  print(digits = 2)
```

The association between binary outcome variable and predictors with more than two categories is statistically significant. The slope of peak exercise (*ST_S*) has a strong relationship with the number of heart problems. Followed by a moderately strong relationship with chest pain types (*CPT*) observed. The least strong relationship is with the results of resting electrocardiogram results (*R_ECG*), which is almost negligible. There is a significant relationship between slope and chest pain type, however it is moderate. 

For binary variables, the same association measure is used. 
```{r, message=FALSE}
train_heart %>%
  select(Sex, F_BS, E_A, H_D) %>% 
  assoc() %>%
  print(digits = 2)
```

The observations of angina induced by exercise (*E_A*) correlate strongly with the heart disease. Gender (*Sex*) seems to have a moderately strong relationship with the heart disease observations. Fasting blood sugar (*F_BS*) has a weak relationship with the outcome variable. 

### Sex
To find the importance of a categorical variable, instead of correlation, we can be looking at how often a heart disease is detected per category. 
```{r}
tsex <- table(train_heart$HeartDisease, train_heart$Sex)
tsexp <- scale(tsex, FALSE, colSums(tsex)) * 100

bp_sex <- barplot(tsexp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("topright", c("Normal","Problem"), fill = c("green","red"))
text(bp_sex, 0, round(tsexp, 0), cex = 1, pos = 3)
```
In this data set men have heart problems more often than women (63% versus 27%). 

```{r}
train_heart %$%
  ctable(HeartDisease, Sex,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )
```
### Chest pain type

```{r}
tCPT <- table(train_heart$HeartDisease, train_heart$ChestPainType)
tCPTp <- scale(tCPT, FALSE, colSums(tCPT)) * 100

bp_CPT <- barplot(tCPTp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("topright", c("Normal","Problem"), fill = c("green","red"))
text(bp_CPT, 0, round(tCPTp, 0), cex = 1, pos = 3)
```

```{r}
train_heart %$%
  ctable(HeartDisease, ChestPainType,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )
```

In this data set asymptomatic chest pain type has relatively more cases which have heart concerns (73%). 
### Fasting blood sugar

```{r}
tFBS <- table(train_heart$HeartDisease, train_heart$FastingBS)
tFBSp <- scale(tFBS, FALSE, colSums(tFBS)) * 100

bp_FBS <- barplot(tFBSp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("topright", c("Normal","Problem"), fill = c("green","red"))
text(bp_FBS, 0, round(tFBSp, 0), cex = 1, pos = 3)
```

```{r}
train_heart %$%
  ctable(HeartDisease, FastingBS,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )

```
Patients with blood sugar above 120 mg/dl have heart disease relatively more often (64% versus 44%). 

### Resting electrocardiogram
```{r}
tRECG <- table(train_heart$HeartDisease, train_heart$RestingECG)
tREp <- scale(tRECG, FALSE, colSums(tRECG)) * 100

bp_RE <- barplot(tREp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("topright", c("Normal","Problem"), fill = c("green","red"))
text(bp_RE, 0, round(tREp, 0), cex = 1, pos = 3)
```

```{r}
train_heart %$%
  ctable(HeartDisease, RestingECG,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )

```

ST and LVH results have more than a half cases with heart problems diagnosed. 
The differences between classes are relatively small, so it can be assumed that the association is weak. 
### Exercise angina

```{r}
tEA <- table(train_heart$HeartDisease, train_heart$ExerciseAngina)
tEAp <- scale(tEA, FALSE, colSums(tEA)) * 100

bp_EA <- barplot(tEAp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("top", c("Normal","Problem"), fill = c("green","red"))
text(bp_EA, 0, round(tEAp, 0), cex = 1, pos = 3)
```

```{r}
train_heart %$%
  ctable(HeartDisease, ExerciseAngina,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )
```

Patients with angina induced by exercise have heart disease relatively more often (83% versus 26%). 
Here the differences are large, so there might be a strong relationship between *ExerciseAngina* and *HeartDisease*. 

### St Slope

```{r}
tST <- table(train_heart$HeartDisease, train_heart$ST_Slope)
tSTp <- scale(tST, FALSE, colSums(tST)) * 100

bp_ST <- barplot(tSTp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("topright", c("Normal","Problem"), fill = c("green","red"))
text(bp_ST, 0, round(tSTp, 0), cex = 1, pos = 3)
```

```{r}
train_heart %$%
  ctable(HeartDisease, ST_Slope,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )
```

The number of patients with heart problems is largest for cases with the flat and down slopes observed, over the three quarters. As the differences between these categories of slopes are large, this suggests a strong relation with the outcome variable. 

## Summing up correlation results

After the exploration made above, we concluded that *ExerciseAngina*, *ST_Slope*, *MaxHR*, *Oldpeak*, *Age* and *MaxHR* are the variables that show strongest correlation with our outcome variable, *HeartDisease*. Therefore, we are going to look for the best model from these variables. 

# Dummy codes

The variable *ExerciseAngima* is dichotomous and *ST_Slope*, categorical. In order to use them as predictors, we need to work with dummy codes. 

```{r}
dummy_heart <- dummy_cols(train_heart, select_columns = c('ExerciseAngina', 'ST_Slope'))
```

Now we will clean up the dataset and see how it look like.

```{r}
# Selecting variables we will use
dummy_heart <- dummy_heart %>% 
               select(Age, MaxHR, Oldpeak,
                      ExerciseAngina, ExerciseAngina_N, ExerciseAngina_Y,
                      ST_Slope, ST_Slope_Up, ST_Slope_Down, ST_Slope_Up, ST_Slope_Flat,
                      HeartDisease)

# Visualizing dataset
head(dummy_heart) %>% 
  knitr::kable(format = "markdown", digits= 1, padding = 30, align = 'c')
```
We will treat *ExerciseAgina_N* (no for exercise angina) as the reference group and, therefore, only the variable *ExerciseAngina_Y* (yes for exercise angina) will be included in the model, as the focal group.  
In the case of *ST_Slope*, *ST_Slope_Up* will be the reference group and, consequently, only *ST_Slope_Down* and *ST_Slope_Flat* will ne included in the model. 


# Model building

Starting with only one predictor, to explore how the variables behave in a simple model. 

## Simple logistic regression

```{r }
set.seed(42)

```

### Age

When regressing *HeartDesease* on *Age*, one can see that both, intercept and the slope for age, are significant, as soon as the p-value is smaller than 0.05. 
```{r}
model_age <- glm(HeartDisease ~ Age, 
              data = dummy_heart, 
              family = "binomial")

summary(model_age)
```
### Old peak

*Oldpeak* has also a pretty small p-value (< 2e-16) for the intercept and slope and, therefore, has a significant effect on *HeartDisease*. 

```{r}
model_oldpeak <- glm(HeartDisease ~ Oldpeak, 
              data = dummy_heart, 
              family = "binomial")

summary(model_oldpeak)
```
### Max HR

In the simple logistic regression, *MaxHR* shows to have a signifcant effect on *HeartDisease*, presenting a very small p-value (<2e-16). 
```{r}
model_maxhr <- glm(HeartDisease ~ MaxHR, 
              data = dummy_heart, 
              family = "binomial")

summary(model_maxhr)
```
### ST_Slope

As said before, the reference group here is *ST_Slope_Up*. Both slopes for *ST_Slope_Down* and *ST_Slope_Flat* have a tiny p-value and, therefore, are siginificantly bigger than zero. 

```{r}
model_stslope <- glm(HeartDisease ~ ST_Slope_Down + ST_Slope_Flat, 
              data = dummy_heart, 
              family = "binomial")

summary(model_stslope)
```
### ExerciseAngima

Here *ExerciseAniga_N* is the reference group. The slope for the focal group, *ExerciseAngina_Y*, has a small p-value (< 2e-16) and, therefore, is significant. 
```{r}
model_angina <- glm(HeartDisease ~ ExerciseAngina_Y, 
              data = dummy_heart, 
              family = "binomial")

summary(model_angina)
```

## Multiple logistic regression

```{r}
fit1 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR, 
              data = train_heart, 
              family = "binomial")

summary(fit1)
```

```{r}
fit2 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR 
                           + ST_Slope, 
              data = train_heart, 
              family = "binomial")

summary(fit2)
```

```{r}
fit3 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR 
                           + ST_Slope
                           + Oldpeak, 
              data = train_heart, 
              family = "binomial")

summary(fit3)
```


```{r}
fit4 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR 
                           + ST_Slope
                           + Oldpeak
                           + Age, 
              data = train_heart, 
              family = "binomial")

summary(fit4)
```

# Model comparison - additive

## Change in deviance

Testing the change in deviance
The improvement is clear until the model 3, when the deviance decreases significantly. Model 4 shows a small decrease in the deviance, but it is doubtful whether it is worth complicating the model by so little improvement.
We will test for information criteria in the next section - a measurement that penalizes the addition of new predictors - to get better insights about that.
```{r}
anova(fit1, fit2, fit3, fit4, test = "Chisq") 
```
## Information criteria
Comparing information criteria.

The second model has an improvement comparing to the first one, as the AIC dropped off. The information criteria, however, increased again in the third model, making clear that the variable age does not worth to added and, therefore, second model is the best one. 
```{r}
AIC(fit1, fit2, fit3, fit4)
```
BIC follows the same narrative as AIC. 
```{r}
BIC(fit1, fit2, fit3)
```
## Cross-validation

Result model 1 here

```{r}
set.seed(42)
cross1 <- CVbinary(fit1, nfolds = 10)
```

Result model 2 here 

```{r}
set.seed(42)
cross2 <- CVbinary(fit2, nfolds = 10)
```
Result model 3 here

```{r}
set.seed(42)
cross3 <- CVbinary(fit3, nfolds = 10)
```
Result model 4 here

```{r}
set.seed(42)
cross4 <- CVbinary(fit4, nfolds = 10)
```

Comaprison all results here

```{r}
c(fit1 = cross1$acc.cv, fit2 = cross2$acc.cv, fit3 = cross3$acc.cv, fit4 = cross4$acc.cv)
```

# Model comparison - interaction
Now we have chosen the best additive model, we will add interactions to try to improve it. 

```{r}
fit5 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR 
                           + ST_Slope
                           + Oldpeak
                           + ExerciseAngina * MaxHR,
              data = train_heart, 
              family = "binomial")

summary(fit5)
```
```{r}
fit6 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR 
                           + ST_Slope
                           + Oldpeak
                           + ExerciseAngina * ST_Slope,
              data = train_heart, 
              family = "binomial")

summary(fit6)
```
```{r}
fit7 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR 
                           + ST_Slope
                           + Oldpeak
                           + ExerciseAngina * Oldpeak,
              data = train_heart, 
              family = "binomial")

summary(fit7)
```

```{r}
fit7 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR 
                           + ST_Slope
                           + Oldpeak
                           + MaxHR * ST_Slope,
              data = train_heart, 
              family = "binomial")

summary(fit7)
```

```{r}
fit8 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR 
                           + ST_Slope
                           + Oldpeak
                           + MaxHR * Oldpeak,
              data = train_heart, 
              family = "binomial")

summary(fit8)
```


```{r}
fit9 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR 
                           + ST_Slope
                           + Oldpeak
                           + ST_Slope * Oldpeak,
              data = train_heart, 
              family = "binomial")

summary(fit9)
```

```{r}
fit10 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR 
                           + ST_Slope
                           + Oldpeak
                           + ST_Slope * Oldpeak
                           + ExerciseAngina * Oldpeak,
              data = train_heart, 
              family = "binomial")

summary(fit10)
```


```{r}
fit11 <- glm(HeartDisease ~ ExerciseAngina
                           + MaxHR 
                           + ST_Slope
                           + Oldpeak
                           + ExerciseAngina * MaxHR
                           + ExerciseAngina * ST_Slope
                           + ExerciseAngina * Oldpeak
                           + MaxHR * ST_Slope
                           + MaxHR * Oldpeak
                           + ST_Slope * Oldpeak,
              data = train_heart, 
              family = "binomial")

summary(fit11)
```

## Change in deviance

```{r}
anova(fit3, fit5, fit6, fit7, fit8, fit9, fit10, fit11,
                                  test = "Chisq") 
```
## Information criteria

```{r}
AIC(fit3, fit5, fit6, fit7, fit8, fit9, fit10, fit11)
```
```{r}
BIC(fit3, fit5, fit6, fit7, fit8, fit9, fit10, fit11)
```
## Cross Validation

Result model 5 here

```{r}
set.seed(42)
cross5 <- CVbinary(fit5, nfolds = 10)
```
Result model 6 here

```{r}
set.seed(42)
cross6 <- CVbinary(fit6, nfolds = 10)
```

Result model 7 here

```{r}
set.seed(42)
cross7 <- CVbinary(fit7, nfolds = 10)
```

Result model 8 here

```{r}
set.seed(42)
cross8 <- CVbinary(fit8, nfolds = 10)
```

Result model 9 here

```{r}
set.seed(42)
cross9 <- CVbinary(fit9, nfolds = 10)
```

Result model 10 here

```{r}
set.seed(42)
cross10 <- CVbinary(fit10, nfolds = 10)
```

Result  model 11 here

```{r}
set.seed(42)
cross11 <- CVbinary(fit11, nfolds = 10)
```

Comparing all the results here

```{r}
c(fit5 = cross5$acc.cv, fit6 = cross6$acc.cv, fit7 = cross7$acc.cv, 
                        fit8 = cross8$acc.cv, fit9 = cross9$acc.cv, 
                        fit10 = cross10$acc.cv, fit11 = cross11$acc.cv)
```

# Cross Validation

# Confusion Matrix

```{r}
probab <- predict(fit3, newdata = train_heart, type = "response")

class <- ifelse(probab > 0.5, "Prob", "Norm") %>% factor()

table(class)
```

```{r}
confusionMatrix(class, reference = train_heart$HeartDisease)
```


# Visualization of the model 

# Checking assumptions 

## Residuals vs Fitted model

## Normal Q-Q plot

## Scale-Location 

## Cook's distance

## Residuals vs Leverage

# Interpreting the final model

# Answering the research question 

# Discussion & Limitations


# Sources
fedesoriano. (2021, September). *Heart Failure Prediction Dataset.* [Data set] Retrieved from https://www.kaggle.com/fedesoriano/heart-failure-prediction.

Hastie, T., James, G., Tibshirani, R. & Witten, D. (2021). *An Introduction to Statistical Learning with Applications in R*. (2nd edition). Springer. Retrieved from https://hastie.su.domains/ISLR2/ISLRv2_website.pdf

Haira, K. (2018, 17 May). *dplyr - advanced-joining* Retrieved from https://rpubs.com/KunalHaria/390063.

Kassambara, A. (2018). Linear Regression Assumptions and Diagnostics in R: Essentials. In: *Statistical Tools for High-Throughput Data Analysis.* (1st edition). Retrieved from http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/#diagnostic-plots.

Klein, A. G., Gerhard, C., Büchner, R. D., Diestel, S. & Schermelleh-Engel, K. (2016). The detection of heteroscedasticity in regression models for psychological data. *Psychological Test and Assessment Modeling, 58*(4), 542-568.

Makowski, D., Ben-Shachar, M. S., Patil, I., & Lüdecke, D. (2019). Methods and Algorithms for Correlation Analysis in R. *Journal of Open Source Software, 5*(51), 2306. https://doi.org/10.21105/joss.02306

Rousselet, G. A., & Wilcox, R. R. (2020). Reaction times and other skewed distributions: problems with the mean and the median. *Meta-Psychology, 4*. 
https://doi.org/10.15626/MP.2019.1630




