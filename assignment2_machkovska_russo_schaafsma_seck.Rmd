---
title: "Logistic regression assignment, group 3"  
author: Elizabeth Machkovska, Letícia Marçal Russo, Madio Seck, William Schaafsma 
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
       highlight: textmate
       theme: flatly
       number_sections: yes
       toc: yes
       toc_float:
         collapsed: yes
         smooth_scroll: no
---

# Context 

# Packages
```{r libraries, warning = FALSE, message = FALSE}
library(ggplot2)
library(tidyr)
library(GGally)
library(readr)
library(dbplyr)
library(tidyverse)
library(magrittr)
library(lattice)
library(correlation)
library(ggcorrplot)
library(see)
library(caret)
library(greybox)
library(summarytools)
library(DAAG)
library(fastDummies)
library(pROC)
library(ROCR)
```

# Research question 

- tell that we are doing an exploratory analysis about the variables that could predict better heart disease. Therefore, we are not working with hypothesis 

- when running test statistics, we will work with an alpha of 0.05 and confidence of 95%.

- our goal is to make good predictions and not focus in inference. Mainly see how our model works in new data. We are will do it testing predictions in a test set that we will create in the beginning of the analysis.  

# Loading the dataset
The data set is derived from Kaggle and is uploaded by fedesoriano (2021). With the following script a distinction is made between continuous (numeric) and categorical variables which are transformed to factor type data. 
```{r}
heart <- read_csv("heart.csv", 
                  col_types = cols(Age = col_number(),
                                   Sex = col_factor(),
                                   ChestPainType = col_factor(),
                                   RestingBP = col_number(),
                                   Cholesterol = col_number(), 
                                   FastingBS = col_factor(), 
                                   RestingECG = col_factor(),
                                   MaxHR = col_number(),
                                   ExerciseAngina = col_factor(),
                                   Oldpeak = col_number(),
                                   ST_Slope = col_factor(), 
                                   HeartDisease = col_factor()
                   )
                  )
heart$FastingBS <- factor(heart$FastingBS,
                                labels = c("B120", "A120"))
heart$HeartDisease <- factor(heart$HeartDisease,
                                labels = c("Norm", "Prob"))
```

# Explanation of the variables

This data set holds 12 variables of which **Heart disease** is our dichotomous outcome variable. A futher elaboration on the variables follows:

**Variable 1**: *Age*. It's a numeric variable of the patients' age expressed in years.  
  

**Variable 2**: *Sex*. It's a categorical variable which holds the groups: F = female & M = male.  
  

**Variable 3**: *ChestPaintype*. It's a categorical variable which contains 4 different types of chest pains.  

*TA*: Typical Angina, a type of chest pain caused by reduced blood flow to the heart.  

*ATA*: Atypical Angina, a type of chest pain that doesn't meet the criteria for angina.  

*NAP*: Non-Anginal Pain, a type of chest pain that is not caused by heart disease or a heart attack. 

*ASY*: Asymptomatic, a type of chest pain that includes no chest pain but there is angina.  


**Variable 4**: *RestingBP*. It's a numeric variable representing patients' resting blood pressure (mmHg). The average blood pressure for an adult male is 120/80 mmHg. Average blood pressure increases when people get older. Furthermore, women's average blood pressure is mostly a little bit lower compared to males. A resting blood pressure > 140 is considered high and may cause heart failure.  


**Variable 5**: *Cholesterol*. It's a numeric variable representing patients' cholesterol levels (mm / dl). A healthy adult has a cholesterol level lower than 200 mm / dl. High cholesterol levels (> 240) is moderate/high and therefore may cause heart failure.  


**Variable 6**: *FastingBS*. It's a dichotomous variable representing patients' fasting blood sugar (mg / dl). Fasting blood sugar levels > 120 mg / dl are coded as a 1. Fasting blood sugar levels < 120 mg / dl are coded as a 0. High levels of blood sugar infer diabetes which may cause heart failure.   


**Variable 7**: *RestingECG*. It's a categorical variable which contains 3 different types of electric activity in the heart.  

*Normal*: The heart is beating in a regular sinus rhythm at a normal pace. No danger whatsoever.   

*ST*: Indicates an abnormality in the heartbeat. It is associated with increased cardiovascular risk.  

*LVH*: This activity in the heart represents valve problems which increasingly thickens the wall of the heart's main pumping chamber. It's the most common cause of high blood pressure which may cause heart failure.  


**Variable 8**: *MaxHR*.  It's a numeric variable that represents the patients' maximum heart rate. Maximum heart rate is related to age since it's calculated by subtracting 220 by one's age. Exceeding the maximum heart rate, depending on one's age, is dangerous for the heart and may induce heart failure. Logically, a 70-year-old patient has a lower maximum heart rate than a 30-year-old patient.


**Variable 9**: *Exercise Angina*.  It's a categorical variable having two categories. This variable takes into account whether people experience angina after exercise. Y = yes and N = no. People that experience angina after exercise may be more vulnerable to heart disease.


**Variable 10**: *Oldpeak*.  It's a numeric variable which indicates that the trace within the ST segment is abnormally below the baseline. The value is measured in depression. ST segment depression correlates to people with angina and thus may predict heart failure.


**Variable 11**: *ST_slope*.  It's a categorical variable which indicates whether the slope between the S and the T is "Down", "Flat" or "Up". Here a downsloping ST segment is very bad since it's a manifestation of severe myocardial ischemia which may induce heart failure.


**Variable 12**: *HeartDisease*.  It's a dichotomous variable and our outcome variable is this assignment. 0 means 'no heart disease / normal' 1 means 'heart disease'

# Inspecting and cleaning up

add information like:
* number of observations

```{r} 
head(heart) %>% 
  knitr::kable(format = "markdown", digits= 1, padding = 30, align = 'c')
```
Apparently, none of the variables have missing values, but we will get more insights in the descriptive analysis.

```{r}
summary(heart)
```
Comment here

```{r}
str(heart)
```

# Descriptive analysis

Comment here.

```{r}
Hist_age <- hist(heart$Age)
```

This is not working. Check why.
```{r}
#Box_age <- boxplot(heart$Age) + title(main = "Distribution of age", ylab = "Age in years")
```

Comment here.

```{r}
median(heart$Age)
```

Age is distributed normally. The youngest observations is 28 years old and the oldest observation is 77 years old. The median of this data set is 54.

```{r}
Hist1_chol <- hist(heart$Cholesterol)
```

The Cholesterol variable has 172 observations of the number 0. It is not possible that a person has zero cholesterol level. Therefore, we will consider this information as error and, consequently, missing values.
In the next topic, we will make a decision about how to deal with that.


```{r}
#Box_chol <- boxplot(heart$Cholesterol) 
#            + title(main = "Distribution of cholesterol levels", 
#                    ylab = "Cholesterol levels (mm / dl)")
```
Comment here.

```{r}
min(heart$Cholesterol)
max(heart$Cholesterol)
median(heart$Cholesterol)
```

Here we see that the distribution of cholesterol levels has several outliers. The median is a cholesterol level of 237 which is within the range of a healthy level. a The lowest level of cholesterol is 85 and the highest level of cholesterol is 603. When checking assumptions we will see whether we need to exclude more observations due to abnormal cholesterol levels.

```{r}
Hist_BPrest <- hist(heart$RestingBP)
```

Comment here.

```{r}
#Box_BPrest <- boxplot(heart$RestingBP) 
#                 + title(main = "Distribution of resting blood pressure levels", 
#                         ylab = "Resting blood pressure levels (mmHg)")
```

Comment here.

```{r}
median(heart$RestingBP)

max(heart$RestingBP)
```

Most of the observations have a healthy resting blood pressure, which the median of 130 also represents. There are some outliers past the maximum of the boxplot, the highest observations has a resting blood pressure of 200 which is very dangerous.

## Missing values for cholesterol

-decision if we delete missing values or delete the variable
- 172 is almost 20% of the dataset (918 observations)
-run a correlation here to see if cholesterol is related to heart disease 
-argument that the best choice was to delete the variable


```{r}
# Excluding the 0 values
heart_cholesterol <- heart %>% 
                     filter(!Cholesterol == 0)
```

Correlation

```{r}
heart_cholesterol %>%
  select(HeartDisease, Cholesterol) %>% 
  correlation(include_factors = TRUE, method = "auto") %>%
  summary() %>%
    plot(show_labels=TRUE,
       show_p=TRUE,
       size_point=5,
       size_text=4,
       digits=2,
       type="tile",
       )
```
Very weak correlation. Decision: exclude the variable.

```{r}
Clean_Heart <- heart %>% 
               select(!Cholesterol)
```

# Split the data: train and test
In the training data set 80% (p=0.8) of the rows were generated as a new sample. Out of 735 observations 45% has normal heart performance (328 rows) and 55% has heart problems (407 rows). Therefore, the training data has approximately equal distribution of classes within response variable. 
```{r}
set.seed(42)

# Split data randomly
trainDataIndex <- createDataPartition(Clean_Heart$HeartDisease, 
                                      p = 0.8, 
                                      list = FALSE) 

# Create training and testing set
train_heart <- Clean_Heart[trainDataIndex, ]
test_heart <- Clean_Heart[-trainDataIndex, ]

table(train_heart$HeartDisease)
```

# Correlation 

The **easystats** package by Makowski and his colleagues (2019) and the **greybox** package by Svetunkov and Yves (2022) is used to create the pairwise correlation matrix. The long variable names are first shortened to make visualization neater. 
```{r}
# Make the names shorter
train_heart$CPT <- train_heart$ChestPainType
train_heart$R_BP <- train_heart$RestingBP
train_heart$F_BS <- train_heart$FastingBS
train_heart$R_ECG <- train_heart$RestingECG
train_heart$E_A <- train_heart$ExerciseAngina
train_heart$ST_S <- train_heart$ST_Slope
train_heart$H_D <- train_heart$HeartDisease

```

## Continuous predictors
Correlation between continuous predictor variables and dichotomous categorical outcome variable is calculated using the *Point-biserial* correlation. Regarding nominal variables, the coefficient does tell something about the strength of a relation, however, not about the direction, even though those are visualised in the matrix for *H_D*. The relationship between continuous predictors is measured using *Pearson*. 
```{r, message=FALSE}
train_heart %>%
  select(Age, R_BP, MaxHR, Oldpeak, H_D) %>% 
  correlation(include_factors = TRUE, method = "auto") %>%
  summary() %>%
    plot(show_labels=TRUE,
       show_p=TRUE,
       size_point=5,
       size_text=4,
       digits=2,
       type="tile",
       )
```


When comparing the coefficients for the observations of patients with a heart disease (*H_D.Prob*), in order from strongest to weakest association, there is a significant strong positive relationship between oldpeak, which means that the higher the value of *O_P*, the more often a heart problem occurs. Relationship between *MaxHR* and heart disease is moderately negative, which suggests that the higher the maximum heart rate achieved the less heart problems are observed. Both *Oldpeak* and *MaxHR* have a moderate strength of a relationship between each other and with *Age*. In turn, *Age* has a moderate positive relationship with heart problems, meaning that the older the patients observed, the more often they have a heart disease detected. Association between resting blood pressure (*R_BP*) is weak and positive. The relationships have a reverse direction when comparing the values for a normal heart (*H_D.Norm*). 

## Categorical predictors
The *Cramer's V* is used for categorical variables. 
```{r, message=FALSE}
train_heart %>%
  select(CPT, R_ECG, ST_S, H_D) %>% 
  assoc() %>%
  print(digits = 2)
```

The association between binary outcome variable and predictors with more than two categories is statistically significant. The slope of peak exercise (*ST_S*) has a strong relationship with the number of heart problems. Followed by a moderately strong relationship with chest pain types (*CPT*) observed. The least strong relationship is with the results of resting electrocardiogram results (*R_ECG*), which is almost negligible. There is a significant relationship between slope and chest pain type, however it is moderate. 

For binary variables, the same association measure is used. 
```{r, message=FALSE}
train_heart %>%
  select(Sex, F_BS, E_A, H_D) %>% 
  assoc() %>%
  print(digits = 2)
```

The observations of angina induced by exercise (*E_A*) correlate strongly with the heart disease. Gender (*Sex*) seems to have a moderately strong relationship with the heart disease observations. Fasting blood sugar (*F_BS*) has a weak relationship with the outcome variable. 

To find the importance of a categorical variable, instead of correlation, we can be looking at how often a heart disease is detected per category. The bar plots depict the proportion of normal or problematic heart performance per class within a group. The cross-tabulation gives the proportion of cases per class per total number of observations of given heart performance. 
### Sex
 
```{r}
tsex <- table(train_heart$HeartDisease, train_heart$Sex)
tsexp <- scale(tsex, FALSE, colSums(tsex)) * 100

bp_sex <- barplot(tsexp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("topright", c("Normal","Problem"), fill = c("green","red"))
text(bp_sex, 0, round(tsexp, 0), cex = 1, pos = 3)
```

In this data set men have heart problems more often than women (63% versus 27%). 

```{r}
train_heart %$%
  ctable(HeartDisease, Sex,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )
```
### Chest pain type

```{r}
tCPT <- table(train_heart$HeartDisease, train_heart$ChestPainType)
tCPTp <- scale(tCPT, FALSE, colSums(tCPT)) * 100

bp_CPT <- barplot(tCPTp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("topright", c("Normal","Problem"), fill = c("green","red"))
text(bp_CPT, 0, round(tCPTp, 0), cex = 1, pos = 3)
```

In this data set atypical angina (*ATA*) chest pain type has relatively more normal heart performance (85%). By contrast, asymptomatic (*ASY*) chest pain type has relatively more cases which have heart concerns (79%). 

```{r}
train_heart %$%
  ctable(HeartDisease, ChestPainType,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )
```

 
### Fasting blood sugar

```{r}
tFBS <- table(train_heart$HeartDisease, train_heart$FastingBS)
tFBSp <- scale(tFBS, FALSE, colSums(tFBS)) * 100

bp_FBS <- barplot(tFBSp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("topright", c("Normal","Problem"), fill = c("green","red"))
text(bp_FBS, 0, round(tFBSp, 0), cex = 1, pos = 3)
```

Patients with blood sugar above 120 mg/dl have heart disease relatively more often (80% versus 48%).The association between the outcome and *A120* should be stronger. 

```{r}
train_heart %$%
  ctable(HeartDisease, FastingBS,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )

```
 

### Resting electrocardiogram
```{r}
tRECG <- table(train_heart$HeartDisease, train_heart$RestingECG)
tREp <- scale(tRECG, FALSE, colSums(tRECG)) * 100

bp_RE <- barplot(tREp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("topright", c("Normal","Problem"), fill = c("green","red"))
text(bp_RE, 0, round(tREp, 0), cex = 1, pos = 3)
```

All the resting electrocardiogram results have more than a half cases with heart problems diagnosed. 
The differences between classes, especially between *Normal* and *LVH* are relatively small, so it can be assumed that the association is weak. 

```{r}
train_heart %$%
  ctable(HeartDisease, RestingECG,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )

```


### Exercise angina

```{r}
tEA <- table(train_heart$HeartDisease, train_heart$ExerciseAngina)
tEAp <- scale(tEA, FALSE, colSums(tEA)) * 100

bp_EA <- barplot(tEAp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("top", c("Normal","Problem"), fill = c("green","red"))
text(bp_EA, 0, round(tEAp, 0), cex = 1, pos = 3)
```

Patients with angina induced by exercise (*Y*) have heart disease relatively more often (85% versus 35%). 
Here the differences are large, so there might be a strong relationship between *ExerciseAngina* and *HeartDisease*. 


```{r}
train_heart %$%
  ctable(HeartDisease, ExerciseAngina,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )
```



### St Slope

```{r}
tST <- table(train_heart$HeartDisease, train_heart$ST_Slope)
tSTp <- scale(tST, FALSE, colSums(tST)) * 100

bp_ST <- barplot(tSTp, beside = TRUE, col = c("green","red"), ylab="Frequency (%)", ylim=c(0,100))
legend("topright", c("Normal","Problem"), fill = c("green","red"))
text(bp_ST, 0, round(tSTp, 0), cex = 1, pos = 3)
```

The proportion of patients with heart problems is largest for cases with the *Flat* and *Down* slopes observed, over the three quarters (82% and 72%). By contrast, the majority of *Up* slope cases are normal (80%). As the differences between these categories of slopes are large, this suggests a strong relation with the outcome variable. 


```{r}
train_heart %$%
  ctable(HeartDisease, ST_Slope,
    prop = "r", chisq = TRUE, headings = FALSE
  ) %>%
  print(
    method = "render",
    style = "rmarkdown",
    footnote = NA
  )
```


## Summing up correlation results

After the exploration made above, we concluded that *ExerciseAngina*, *ST_Slope*, *MaxHR*, *Oldpeak*, *Age* and *MaxHR* are the variables that show strongest correlation with our outcome variable, *HeartDisease*. Therefore, we are going to look for the best model from these variables. 

# Dummy codes

The variable *ExerciseAngina* is dichotomous and *ST_Slope*, categorical. In order to use them as predictors, we need to work with dummy codes. 

```{r}
dummy_heart <- dummy_cols(train_heart, select_columns = c('ExerciseAngina', 'ST_Slope'))
```

Now we will clean up the dataset and see how it look like.

```{r}
# Selecting variables we will use
dummy_heart <- dummy_heart %>% 
               select(Age, MaxHR, Oldpeak,
                      ExerciseAngina, ExerciseAngina_N, ExerciseAngina_Y,
                      ST_Slope, ST_Slope_Up, ST_Slope_Down, ST_Slope_Flat,
                      HeartDisease)

# Visualizing dataset
head(dummy_heart) %>% 
  knitr::kable(format = "markdown", digits= 1, padding = 30, align = 'c')
```
We will treat *ExerciseAgina_N* (no for exercise angina) as the reference group and, therefore, only the variable *ExerciseAngina_Y* (yes for exercise angina) will be included in the model, as the focal group.  
In the case of *ST_Slope*, *ST_Slope_Up* will be the reference group and, consequently, only *ST_Slope_Down* and *ST_Slope_Flat* will ne included in the model. 


# Additive models 

Starting with only one predictor, to explore how the variables behave in a simple model. 

## Simple logistic regression

```{r }
set.seed(42)

```

### Age

When regressing *HeartDesease* on *Age*, one can see that both, intercept and the slope for age, are significant, as soon as the p-value is smaller than 0.05. 
```{r}
model_age <- glm(HeartDisease ~ Age, 
              data = dummy_heart, 
              family = "binomial")

summary(model_age)
```
### Old peak

*Oldpeak* has also a pretty small p-value (< 2e-16) for the intercept and slope and, therefore, has a significant effect on *HeartDisease*. 

```{r}
model_oldpeak <- glm(HeartDisease ~ Oldpeak, 
              data = dummy_heart, 
              family = "binomial")

summary(model_oldpeak)
```
### Max HR

In the simple logistic regression, *MaxHR* shows to have a signifcant effect on *HeartDisease*, presenting a very small p-value (<2e-16). 
```{r}
model_maxhr <- glm(HeartDisease ~ MaxHR, 
              data = dummy_heart, 
              family = "binomial")

summary(model_maxhr)
```
### ST_Slope

As said before, the reference group here is *ST_Slope_Up*. Both slopes for *ST_Slope_Down* and *ST_Slope_Flat* have a tiny p-value and, therefore, are siginificantly bigger than zero. 

```{r}
model_stslope <- glm(HeartDisease ~ ST_Slope_Down + ST_Slope_Flat, 
              data = dummy_heart, 
              family = "binomial")

summary(model_stslope)
```
### ExerciseAngima

Here *ExerciseAniga_N* is the reference group. The slope for the focal group, *ExerciseAngina_Y*, has a small p-value (< 2e-16) and, therefore, is significant. 
```{r}
model_angina <- glm(HeartDisease ~ ExerciseAngina_Y, 
              data = dummy_heart, 
              family = "binomial")

summary(model_angina)
```

## Multiple logistic regression

Now we will start adding up new predictors, one by one, until the five variables are in the model. For the sake of the organization, we will train all the models before and pay attention to their p-values. In the next topic, we will make the comparison among all the four models.  

First one will contain the predictors *ExerciseAngina_Y* and *MaxHR*. As seen in the output bellow, the intercept and the slopes for both of the predictors have tiny p-value and, therefore, are significant. 
```{r}
fit1 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR, 
              data = dummy_heart, 
              family = "binomial")

summary(fit1)
```

In the second model, the categorical predictor *ST_slope*, presented as the dummy codes *ST_Slope_Down* and *ST_Slope_Flat*, will be added in the model. All the slopes are significant, but the intercept has a p-value bigger than 0.05. 
```{r}
fit2 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR 
                           + ST_Slope_Down
                           + ST_Slope_Flat, 
              data = dummy_heart, 
              family = "binomial")

summary(fit2)
```

The numeric predictor *Oldpeak* is the next one to join the model. As **fit2**, this model has significant slopes, but a non-significant intercept. 
```{r}
fit3 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR 
                           + ST_Slope_Down
                           + ST_Slope_Flat
                           + Oldpeak, 
              data = dummy_heart, 
              family = "binomial")

summary(fit3)
```
Lastly, the numeric predictor *Age* will be added. It does not show a significant slope (p-value = 0.201367). We will check in the next topic if it is worth adding *Age* to the model. 

```{r}
fit4 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR 
                           + ST_Slope_Down
                           + ST_Slope_Flat
                           + Oldpeak
                           + Age, 
              data = dummy_heart, 
              family = "binomial")

summary(fit4)
```

# Additive models comparison 

## Change in deviance

We will first test the change in deviance in order to choose the best model. As soon as we are talking about deviance, we should choose the model that has the smallest result for the residual deviance.  
When we use the function anova, we compare the change between **fit1** and **fit2**; then between **fit2** and **fit3**; and lastly, between **fit3** and **fit4**.   
The improvement is clear until **fit3**, when the deviance decreases significantly. **Fit4** shows a small decrease in the deviance, but it is not significant (p-value = 0.201).  
By the criterion of parsimony, it is not worth adding complexity to the model, if the decrease in the deviance is not significant. *Age* should not be added and, therefore, *fit3* is the best for now. 

```{r}
anova(fit1, fit2, fit3, fit4, test = "Chisq") 
```

## Information criteria

Now we will compare the model by means of information criteria. These measurements penalize the addition of new predictors. The best model is the one that shows a smaller number for AIC and BIC.  
The output bellow shows that **fit2** has an improvement compared to **fit1** as well as *fit3* presents an improvement compared to *fit2*.  
AIC, however, increases for *fit4*, confirming that *Age* is not worth being added.

```{r}
AIC(fit1, fit2, fit3, fit4)
```
BIC follows the same narrative as AIC and, therefore, *fit3* is the best model.  
```{r}
BIC(fit1, fit2, fit3, fit4)
```

# Moderated models 

After choosing **fit3** as the best additive model, we will add interactions to try to improve it. As it was done before, we will train all the models before, analyse the p-values and, later on, make the model comparison.  

We will start with the interaction between the two first predictors, *ExerciseAngina_Y* and *MaxHR*. 
The output bellow shows that after adding the new term, the predictor *ExerciseAngina_T* is no longer significant (p-value = 0.102). The interaction *ExerciseAngina_Y* and *MaxHR* is also not significant (p-value: 0.511).

```{r}
fit5 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR 
                           + ST_Slope_Down
                           + ST_Slope_Flat
                           + Oldpeak
                           + ExerciseAngina_Y * MaxHR,
              data = dummy_heart, 
              family = "binomial")

summary(fit5)
```
After adding an interaction term between *ExerciseAngina_Y* and *ST_Slope_Down* and another one between *ExerciseAngina_Y* and *ST_Slope_Flat*, we see that *ST_Slope_Down* itself is no longer significant (p-value = 0.441). The two interaction terms are also not significant. 

```{r}
fit6 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR 
                           + ST_Slope_Down
                           + ST_Slope_Flat
                           + Oldpeak
                           + ExerciseAngina_Y*ST_Slope_Down
                           + ExerciseAngina_Y*ST_Slope_Flat,
              data = dummy_heart, 
              family = "binomial")

summary(fit6)
```
The output shows that the interaction between *ExerciseAngina_Y* and *Oldpeak* is not significant (p-value = 0.457).

```{r}
fit7 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR 
                           + ST_Slope_Down
                           + ST_Slope_Flat
                           + Oldpeak
                           + ExerciseAngina_Y * Oldpeak,
              data = dummy_heart, 
              family = "binomial")

summary(fit7)
```
It does not seem that adding interaction between *MaxHR* and *ST_Slope* makes the model better. After doing that, *MaxHR*, *ST_Slope_Down* and *ST_Slope_Flat* are no longer significant. The interaction term *MaxHR* and *ST_Slope_Down* is not significant (p-value = 0.311), neither is *MaxHR* and *ST_Slope_Flat* (p-value = 0.054).

```{r}
fit8 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR 
                           + ST_Slope_Down
                           + ST_Slope_Flat
                           + Oldpeak
                           + MaxHR * ST_Slope_Down
                           + MaxHR * ST_Slope_Flat,
              data = dummy_heart, 
              family = "binomial")

summary(fit8)
```
The interaction between *MaxHR* and *Oldpeak* is not significant (p-value = 0.474) and, after adding it, *Oldpeak* is not significant anymore (p-value = 0.906).   

```{r}
fit9 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR 
                           + ST_Slope_Down
                           + ST_Slope_Flat
                           + Oldpeak
                           + MaxHR * Oldpeak,
              data = dummy_heart, 
              family = "binomial")

summary(fit9)
```
A significant interaction was found. The interaction term *ST_Slope_Down* and *Oldpeak* has a p-value of 0.0266, and *ST_Slope_Flat* and *Oldpeak*, a very tiny one (2.91e-05).

```{r}
fit10 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR 
                           + ST_Slope_Down
                           + ST_Slope_Flat
                           + Oldpeak
                           + ST_Slope_Down * Oldpeak
                           + ST_Slope_Flat * Oldpeak,
              data = dummy_heart, 
              family = "binomial")

summary(fit10)
```
And lastly, we will train a model with interaction terms for all the predictors. The output is not good: most of the slopes are not significant anymore. 

```{r}
fit11 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR 
                           + ST_Slope_Down
                           + ST_Slope_Flat
                           + Oldpeak
                           + ExerciseAngina_Y * MaxHR
                           + ExerciseAngina_Y * ST_Slope_Down
                           + ExerciseAngina_Y * ST_Slope_Flat
                           + ExerciseAngina_Y * Oldpeak
                           + MaxHR * ST_Slope_Down
                           + MaxHR * ST_Slope_Flat
                           + MaxHR * Oldpeak
                           + ST_Slope_Down * Oldpeak
                           + ST_Slope_Flat * Oldpeak,
              data = dummy_heart, 
              family = "binomial")

summary(fit11)
```

# Moderated models comparison 

Now we trained the models with interaction terms, we will make the comparison.  

## Change in deviance

We will compare the *fit3*, the best additive model, with the ones with interaction. The output shows that the change in the variance for models *fit3*, *fit5*, *fit6*, *fit7*, *fit8* and *fit9* are all between 616 and 620. Sometimes it goes up and then down, but they don't leave this range.   
However, the residual deviance of *fit10* drop to  602.47 and it is significant (p-value = 2.369e-05). *Fit11*, the one with all interaction terms, shows a small improvement in the deviance (594.26), but it is not significant (p-value = 0.314). Due to the parsimony criterion, we will consider *fit10* the best model according to the change in deviance criterion.  

```{r}
anova(fit3, fit5, fit6, fit7, fit8, fit9, fit10, fit11,
                                  test = "Chisq") 
```
## Information criteria

According to AIC, *fit10* is also the preferred model, because It has the smallest number (618.47) among all the models. 

```{r}
AIC(fit3, fit5, fit6, fit7, fit8, fit9, fit10, fit11)
```
BIC presents the same analysis: *fit10* has the smallest number (655.27) and is, therefore, the favorite model.  
It is interesting to notice that BIC is more conservative than AIC. *Fit11*, than is the most complex one, shows a considerable increase in its BIC (693.25), even stronger than the rise we see in AIC. 

```{r}
BIC(fit3, fit5, fit6, fit7, fit8, fit9, fit10, fit11)
```

# Checking assumptions 

## Residuals vs Fitted model

## Normal Q-Q plot

## Scale-Location 

## Cook's distance

## Residuals vs Leverage

## Influential cases

# Predictions 

Now we will start working with the test set. We will make predictions in the probability scale plugging in our best model *fit10* to the test set.  

After getting the probabilities prediction, we need to set a threshold in order to classify the outcomes in the two groups of heart disease: the normal group (the ones with healthy heart) and the problematic group (the ones with heart disease).  

We will use the 50% probability as the cut off. As we are trying to predict heart disease, we will consider the success group as the problematic one ("Prob").  

If the (predicted) probability is higher than 50%, the person will be classified as having heart disease; if the (predicted) probability is lower or equal to 50%, the person is classified in the group of normal hearts. 
```{r}
# Creating the dummy codes in the test set
test_heart_dummy <- dummy_cols(test_heart, 
                        select_columns = c('ExerciseAngina', 'ST_Slope'))

# Predictions in the probability scale
probab <- predict(fit10, newdata = test_heart_dummy, type = "response")

# Add column of predictions to the test set
test_heart_dummy$prediction <- probab

# Setting a cut off at 50%
class <- ifelse(probab > 0.5, "Prob", "Norm") %>% factor()

# Add column of predicted classification to the test set
test_heart_dummy$classification <- class

# Selecting variables to visualize 
test_heart_pred <- test_heart_dummy %>% 
                   select(MaxHR, Oldpeak, ExerciseAngina_Y, ST_Slope_Down, 
                          ST_Slope_Flat, HeartDisease, prediction, classification)

# Visualizing dataset
head(test_heart_pred) %>% 
  knitr::kable(format = "markdown", digits= 1, padding = 30, align = 'c')

```

# Confusion Matrix

We will check the out of sample performance of our model with a confusion matrix. The dataset was split in test and train sets in the beginning of the assignment. No we work with the test set to see what is the accuracy of *fit10* in a different sample than the one in which we trained the model.  

The test set has 183 observations, of which 66 are true negatives; 91 are true positives; 10 are false negatives; and 16 are false positives.  

With this result, we can interpret the *sensitivity*, *specificity* and *accuracy*. Before doing so, it is important to pay attention to the last line of the output.   

"'Positive' Class : Norm" means that the machine is considering *Norm* as the success group. As said in the topic above, our analysis consider *Prob* as the success group, as soon as we are predicting heart disease -but R does not know it. Because of that, we will have to switch the output for specificity and sensitivity.

```{r}
confusionMatrix(class, reference = test_heart_dummy$HeartDisease)
```
## Sensitivity 

Sensitivity is a measure of how well we can predict membership in the positive class or, in other words, the true positives. Our model *fit10* has a sensitivity equal to 0.901. This means that there is a 90,1% probability that the model will predict heart disease when the person inideed has heart problems. 

## Specificity

On the other hand, specificity is a measure of how well we can predict a negative classification or, in other words, the true negatives. It is important to say that sensitivity and specificity are not complementary (they do not sum up 100% together), and a good model should have a high sensitivity and a high specificity.  

*Fit10* has a specificity of 0.8049. This means that there is a 80,49% chance that the model will indicate no heart disease when the person indeed has a normal heart. 

## Accuracy

Accuracy is the proportion of cases we correctly classified: it is the sum of the true positive and true negatives divided total result. *Fit10* has an accuracy of 0.8579, which means that 85,79% chance of making a true prediction, whether negative or positive.

## Error rate

The error rate speaks for itself and it is the proportion of cases that the model incorrectly predicts. Accuracy is complementary to accuracy: it is calculated by 1 - accuracy. it also can be calculated as the sum of false positives and false negatives divided by the total result.  

*Fit10* has an error rate of 0.1421 and, therefore, has 14,21% probability of making a false prediction.  

A good model has a high accuracy and low error rate. 

## ROC Curve

```{r}

# Duplicate HeartDisease column
test_heart_pred$prob_heart <- test_heart_pred$HeartDisease    

#levels(test_heart_pred$HeartDisease) <- c(1,0)
#names(test_heart_pred)[10] <- "prob_heart"


#test_heart_pred$HeartDisease <- as.factor(test_heart_pred$HeartDisease) 
#test_heart_pred$prob_heart <- as.numeric(test_heart_pred$HeartDisease)



#roc_curve <- performance(pred, measure = "tpr", x.measure = "fpr")

#plot(roc_curve, colorize = T, lwd = 2)

#abline(a = 0, b = 1) 
```

# Visualization of the model 

To visualise the probability of heart disease the regression coefficients in terms of *log odds*, will be exponentiated to *odds ratios* and then transformed into *probabilities*. The code for the plots is derived from R Club (2016). 

These are the regression estimates which came out of the **fit10** expressed in *log odds*. 
```{r}
fit10$coefficients
```

Saving the *log odds* coefficients for the equations.
```{r}
b0 <- fit10$coefficients[1]
EAy <- fit10$coefficients[2]
MH <- fit10$coefficients[3]
STd <- fit10$coefficients[4]
STf <- fit10$coefficients[5]
OP <- fit10$coefficients[6]
STd.OP <- fit10$coefficients[7]
STf.OP <- fit10$coefficients[8]
```

There are two continuous predictor variables against which the probability of the outcome can be plotted against. The first one is *MaxHR* and the other one is *Oldpeak*. 

## MaxHR as x and Oldpeak as mean

Entering the range of the predictor on the x-axis, in this case *MaxHR*. 
```{r}
MH_range <- seq(from=min(dummy_heart$MaxHR), to=max(dummy_heart$MaxHR), by=.01)
```

By plugging in the mean as value of *Oldpeak*, the plots will be generated to show the relationship between *MaxHR* and the outcome for a patient with an average oldpeak. 
```{r}
OP_val <- mean(dummy_heart$Oldpeak)
```

Computing the regression equations for each class of *ST_Slope* in logit terms.
```{r}
#The reference group - ST_Slope_Up
up_logits <- b0 +
  MH*MH_range +
  OP*OP_val +
  EAy*0 +
  STd*0 +
  STf*0 +
  STd.OP*MH_range*0 +
  STf.OP*MH_range*0 +
  STd.OP*OP_val*0 +
  STf.OP*OP_val*0

#The focal groups
down_logits <- b0 +
  MH*MH_range +
  OP*OP_val +
  EAy*0 +
  STd*1 +
  STf*0 +
  STd.OP*MH_range*1 +
  STf.OP*MH_range*0 +
  STd.OP*OP_val*1 +
  STf.OP*OP_val*0

flat_logits <- b0 +
  MH*MH_range +
  OP*OP_val +
  EAy*0 +
  STd*0 +
  STf*1 +
  STd.OP*MH_range*0 +
  STf.OP*MH_range*1 +
  STd.OP*OP_val*0 +
  STf.OP*OP_val*1
```

Computing the probabilities which are going to be plotted.
```{r}
up_probs <- exp(up_logits)/(1 + exp(up_logits))
down_probs <- exp(down_logits)/(1 + exp(down_logits))
flat_probs <- exp(flat_logits)/(1 + exp(flat_logits))
```

Making the graph of probabilities of heart disease against *MaxHR* and an average value of *Oldpeak* for *ST_Slope* groups.
```{r}
#Plot for the reference group
plot(MH_range, up_probs,
     ylim=c(0,1),
     type="l",
     lwd=3,
     lty=2,
     col="gold",
     xlab="MaxHR", ylab="Probability of Heart Disease")

#Add the line for down slope
lines(MH_range, down_probs,
      type="l",
      lwd=3,
      lty=2,
      col="darkgreen")

#Add the line for flat slope
lines(MH_range, flat_probs,
      type="l",
      lwd=3,
      lty=4,
      col="red")

#Add a horizontal line at p=0.5
abline(h=0.5, lty=2)

legend(160, 1, 
       c("Up", "Down", "Flat"),
       lty=c(1,1,1),
       lwd=c(2,2,2), col=c("gold", "darkgreen", "red")
       )
```

Computing the regression equations for each class of *ExerciseAngina* in logit terms.
```{r}
#The reference group - ExerciseAngina_N
no_logits <- b0 +
  MH*MH_range +
  OP*OP_val +
  EAy*0 +
  STd*0 +
  STf*0 +
  STd.OP*MH_range*0 +
  STf.OP*MH_range*0 +
  STd.OP*OP_val*0 +
  STf.OP*OP_val*0

#The focal group - ExerciseAngina_Y
yes_logits <- b0 +
  MH*MH_range +
  OP*OP_val +
  EAy*1 +
  STd*0 +
  STf*0 +
  STd.OP*MH_range*0 +
  STf.OP*MH_range*0 +
  STd.OP*OP_val*0 +
  STf.OP*OP_val*0
```

Computing the probabilities which are going to be plotted.
```{r}
no_probs <- exp(no_logits)/(1 + exp(no_logits))
yes_probs <- exp(yes_logits)/(1 + exp(yes_logits))
```

Making the graph of probabilities of heart disease against *MaxHR* and an average value of *Oldpeak* for *ExerciseAngina* groups.
```{r}
#Plot for the reference group
plot(MH_range, no_probs,
     ylim=c(0,1),
     type="l",
     lwd=3,
     lty=2,
     col="blue",
     xlab="MaxHR", ylab="Probability of Heart Disease")

#Add the line for exercise angina yes-values
lines(MH_range, yes_probs,
      type="l",
      lwd=3,
      lty=3,
      col="lightblue")

#Add a horizontal line at p=0.5
abline(h=0.5, lty=2)

legend(160, 1, 
       c("No", "Yes"),
       lty=c(1,1),
       lwd=c(3,3), col=c("blue", "lightblue")
       )
```

## Oldpeak as x and MaxHR as mean

Entering the range of the second predictor on the x-axis, in this case *Oldpeak*. 
```{r}
OP_range <- seq(from=min(dummy_heart$Oldpeak), to=max(dummy_heart$Oldpeak), by=.01)
```

By plugging in the mean as value of *MaxHR*, the plots will be generated to show the relationship between *Oldpeak* and the outcome for a patient with an average detected maximum heart rate. 
```{r}
MH_val <- mean(dummy_heart$MaxHR)
```

Computing the regression equations for each class of *ST_Slope* in logit terms.
```{r}
#The reference group - ST_Slope_Up
up_logits <- b0 +
  OP*OP_range +
  MH*MH_val +
  EAy*0 +
  STd*0 +
  STf*0 +
  STd.OP*OP_range*0 +
  STf.OP*OP_range*0 +
  STd.OP*MH_val*0 +
  STf.OP*MH_val*0

#The focal groups
down_logits <- b0 +
  OP*OP_range +
  MH*MH_val +
  EAy*0 +
  STd*1 +
  STf*0 +
  STd.OP*OP_range*1 +
  STf.OP*OP_range*0 +
  STd.OP*MH_val*1 +
  STf.OP*MH_val*0

flat_logits <- b0 +
  OP*OP_range +
  MH*MH_val +
  EAy*0 +
  STd*0 +
  STf*1 +
  STd.OP*OP_range*0 +
  STf.OP*OP_range*1 +
  STd.OP*MH_val*0 +
  STf.OP*MH_val*1
```

Computing the probabilities which are going to be plotted.
```{r}
up_probs <- exp(up_logits)/(1 + exp(up_logits))
down_probs <- exp(down_logits)/(1 + exp(down_logits))
flat_probs <- exp(flat_logits)/(1 + exp(flat_logits))
```

Making the graph of probabilities of heart disease against *Oldpeak* and an average value of *MaxHR* for *ST_Slope* groups.
```{r}
#Plot for the reference group
plot(OP_range, up_probs,
     ylim=c(0,1),
     type="l",
     lwd=3,
     lty=2,
     col="gold",
     xlab="Oldpeak", ylab="Probability of Heart Disease")

#Add the line for down slope
lines(OP_range, down_probs,
      type="l",
      lwd=3,
      lty=2,
      col="darkgreen")

#Add the line for flat slope
lines(OP_range, flat_probs,
      type="l",
      lwd=3,
      lty=4,
      col="red")

#Add a horizontal line at p=0.5
abline(h=0.5, lty=2)

legend(3.5, 0.6, 
       c("Up", "Down", "Flat"),
       lty=c(1,1,1),
       lwd=c(2,2,2), col=c("gold", "darkgreen", "red")
       )
```


Computing the regression equations for each class of *ExerciseAngina* in logit terms.
```{r}
#The reference group - ExerciseAngina_N
no_logits <- b0 +
  OP*OP_range +
  MH*MH_val +
  EAy*0 +
  STd*0 +
  STf*0 +
  STd.OP*OP_range*0 +
  STf.OP*OP_range*0 +
  STd.OP*MH_val*0 +
  STf.OP*MH_val*0

#The focal group - ExerciseAngina_Y
yes_logits <- b0 +
  OP*OP_range +
  MH*MH_val +
  EAy*1 +
  STd*0 +
  STf*0 +
  STd.OP*OP_range*0 +
  STf.OP*OP_range*0 +
  STd.OP*MH_val*0 +
  STf.OP*MH_val*0
```

Computing the probabilities which are going to be plotted.
```{r}
no_probs <- exp(no_logits)/(1 + exp(no_logits))
yes_probs <- exp(yes_logits)/(1 + exp(yes_logits))
```

Making the graph of probabilities of heart disease against *Oldpeak* and an average value of *MaxHR* for *ExerciseAngina* groups.
```{r}
#Plot for the reference group
plot(OP_range, no_probs,
     ylim=c(0,1),
     type="l",
     lwd=3,
     lty=2,
     col="blue",
     xlab="Oldpeak", ylab="Probability of Heart Disease")

#Add the line for exercise angina yes-values
lines(OP_range, yes_probs,
      type="l",
      lwd=3,
      lty=3,
      col="lightblue")

#Add a horizontal line at p=0.5
abline(h=0.5, lty=2)

legend(3.5, 0.75, 
       c("No", "Yes"),
       lty=c(1,1),
       lwd=c(3,3), col=c("blue", "lightblue")
       )
```


## Alternative visualisation

This visualisation which Kyle used is not giving flat lines for our regression. 
### Augment the data with fitted values from *fit10*
```{r}
tmp <- predict(fit10, type = "link", se.fit = TRUE)
dummy_heart$eta <- tmp$fit
dummy_heart$se <- tmp$se.fit
dummy_heart$pi <- plogis(tmp$fit)

dummy_heart %<>%
  mutate(etaLower = eta - 1.96 * se,
         etaUpper = eta + 1.96 * se,
         piLower = plogis(etaLower),
         piUpper = plogis(etaUpper)
         )
```

### Logit of the *fit10*
```{r}
ggplot(dummy_heart, aes(x = MaxHR, y = eta)) +
  geom_line(aes(color = ST_Slope), lwd = 1) +
  geom_ribbon(aes(ymin = etaLower, ymax = etaUpper, fill = ST_Slope), alpha = 0.2) +
  ylab("Logit of Heart Disease") +
  facet_wrap(vars(ExerciseAngina))
```


### Probability of *fit10*
```{r}
ggplot(dummy_heart, aes(x = Oldpeak, y = pi)) + 
  geom_ribbon(aes(ymin = piLower, ymax = piUpper, fill = ExerciseAngina), alpha = 0.2) +
  geom_line(aes(color = ExerciseAngina), lwd = 1) + 
  ylab("Probability of Survival") +
  facet_wrap(vars(ST_Slope))
```


# Interpreting the final model
```{r}
fit10 <- glm(HeartDisease ~ ExerciseAngina_Y
                           + MaxHR 
                           + ST_Slope_Down
                           + ST_Slope_Flat
                           + Oldpeak
                           + ST_Slope_Down * Oldpeak
                           + ST_Slope_Flat * Oldpeak,
              data = dummy_heart, 
              family = "binomial")

summary(fit10)
```
[Interpretation coefficients in log odds]

The coefficients expressed in odds ratio and given confidence interval of 95%.
```{r, message=FALSE}
exp(cbind(OddsRatio = coef(fit10), confint(fit10)))
```

[Interpretation odds ratio]

Predicted probability of heart disease at each value of *ExerciseAngina* and *ST_Slope* holding *MaxHR* and *Oldpeak* at their means. -> Can't figure out the code for this one: https://stats.oarc.ucla.edu/r/dae/logit-regression/
```{r}
#heartp <- with(dummy_heart, data_frame(MH = mean(MaxHR), OP = mean(Oldpeak), EAy = factor(ExerciseAngina_Y), STd = factor(ST_Slope_Down), STf = factor(ST_Slope_Flat)))

#heartp$Probability <- predict(fit10, newdata = heartp, type = "response")

#heartp
```

# Answering the research question 

# Discussion & Limitations
The distribution of observations per class in categorical variables was not equal for some predictors. However, that might correspond with the rareness of types occurring in real life medical cases. 

# Sources
fedesoriano. (2021, September). *Heart Failure Prediction Dataset.* [Data set] Retrieved from https://www.kaggle.com/fedesoriano/heart-failure-prediction.

Hastie, T., James, G., Tibshirani, R. & Witten, D. (2021). *An Introduction to Statistical Learning with Applications in R*. (2nd edition). Springer. Retrieved from https://hastie.su.domains/ISLR2/ISLRv2_website.pdf

Haira, K. (2018, 17 May). *dplyr - advanced-joining* Retrieved from https://rpubs.com/KunalHaria/390063.

Kassambara, A. (2018). Linear Regression Assumptions and Diagnostics in R: Essentials. In: *Statistical Tools for High-Throughput Data Analysis.* (1st edition). Retrieved from http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/#diagnostic-plots.

Klein, A. G., Gerhard, C., Büchner, R. D., Diestel, S. & Schermelleh-Engel, K. (2016). The detection of heteroscedasticity in regression models for psychological data. *Psychological Test and Assessment Modeling, 58*(4), 542-568.

Makowski, D., Ben-Shachar, M. S., Patil, I., & Lüdecke, D. (2019). Methods and Algorithms for Correlation Analysis in R. *Journal of Open Source Software, 5*(51), 2306. https://doi.org/10.21105/joss.02306

R Club (2016, April 5). *Plotting your logistic regression models.* [Blog] Retrieved from https://blogs.uoregon.edu/rclub/2016/04/05/plotting-your-logistic-regression-models/. 

Rousselet, G. A., & Wilcox, R. R. (2020). Reaction times and other skewed distributions: problems with the mean and the median. *Meta-Psychology, 4*. 
https://doi.org/10.15626/MP.2019.1630

Svetunkov, I. & Yves, R. (2022). *Package ‘greybox’.* Retrieved from https://cran.r-project.org/web/packages/greybox/greybox.pdf. 



